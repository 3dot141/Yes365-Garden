#ai 

> [Ep.1 万字科普ChatGPT-4为何会颠覆人类社会](https://www.modevol.com/episode/clf9d5kni0zo301mm6tkl9t87)

首先是这项技术的底层原理。视频将逐一介绍它的「实质功能」「训练方式」「长板」和「短板」。

## 2.1 实质功能

尽管 ChatGPT 展现出的能力很惊人，但它也没有大众想得那么神，它没有意识，没有欲望，没有情绪，甚至都不理解自己说了什么，就像一只会学话的鹦鹉。

ChatGPT 的实质功能非常简单，四个字就能概括：「单字接龙」。具体来说就是：给它「任意长的上文」，它会用「自己的模型」去生成「下一个字」。

例如，当给它「“我”」这个上文时，它可能会生成「“是”」。

当给它「“我是”」这个上文时，它可能会生成「“一”」。

ChatGPT 本身能做的就只有「生成下一个字」，你所看到的回答全都是用「同一个模型」，根据不同的「上文」生成出来的。

> 那它是怎么回答那些长内容的呢？  
> 答案是：把它自己生成的「下一个字」和「之前的上文」组合成「新的上文」，再让它以此生成「下一个字」。不断重复，就可以生成「任意长的下文」了。该过程也叫“自回归生成”。

例如，当它根据「“我是”」生成了「“一”」之后，把新生成的「“一”」和之前的「“我是”」组合成新的上文，再让它计算「“我是一”」后面接什么字。

假设这次它生成的是「“只”」，那再把「“只”」和「“我是一”」组合起来，让它计算「“我是一只”」后面接什么字。不断重复，就能生成“我是一只小小鸟”了。

## 2.2 训练方式

影响 ChatGPT 生成结果的因素主要有两个：除了「上文」外，另一个就是它的「模型」本身。「模型」就相当于「ChatGPT 的大脑」。即使把同一个上文，送给不同的模型，也会生成不同的结果。

就好比这两只鹦鹉，同样是听到「“我”」这个上文，一只会接「“是”」，而另一只会接「“爱”」，因为两只鸟的主人，一人教的是《我是一只小小鸟》，另一人教的是《我爱你中国》。

为了让「ChatGPT」生成我们想要的结果，而非胡乱生成，就要提前训练「ChatGPT 的大脑」，也就是训练它的模型。

训练方式是：让它遵照所给的「学习材料」来做单字接龙。通过不断调整模型，使得给模型「学习材料的上文」后，模型能生成「对应的下一个字」。

例如，当我们想把《登鹳雀楼》作为「学习材料」来训练 ChatGPT 时，就不断调整「它的模型」，使得：

- 给它「“白”」，它能生成「“日”」
- 给它「“白日”」，它能生成「“依”」
- 给它「“白日依”」，它能生成「“山”」
- 一直到，给它「“白日依山尽，黄河入海流。欲穷千里目，更上一层”」，它能生成「“楼”」。

没学习前，它原本会胡乱生成，但学习后就可以在看到“白日依山尽”时，生成“黄河入海流”了。

> 那如果同时训练了「“白日依山尽，黄河入海流”」和「“白日何短短，百年苦易满”」，再遇到「“白日”」时，会怎么生成「下一个字」？  

> 答案是：按照概率来抽样。有可能会生成「“依”」，也有可能生成「“何”」。

实际上 ChatGPT 给出的结果长这样，也就是「所有字的概率分布」，「生成的下一个字」就是按照概率分布抽样得到的结果。

由于抽样结果具有随机性，所以 ChatGPT 的回答并不是每次都一样。

> 不过，这样训练后，无非就是能补全和续写，那 ChatGPT 又是怎么回答问题的呢？  

> 其实仅靠单字接龙就能回答问题。因为提问和回答也都是文字，可以将二者组合成一个「问答范例（学习材料）」让 ChatGPT 做单字接龙。

例如，当我们想让 ChatGPT 学习「怎么回答白日依山尽的下一句」时，就可以把「这个提问」和「正确回答」组合成一个「问答范例（学习材料）」，让它按照“请问白日依山尽的下一句是什么 白日依山尽的下一句是黄河入海流”来做单字接龙。

这样以来，当用户输入“请问白日依山尽的下一句是什么”时，它就能生成“白日依山尽的下一句是黄河入海流”了。

## 2.3 长板

但提问和回答的方式无穷无尽，像上面的提问还可以是：

- “白日依山尽的下一句？”
- “白日依山尽的后续内容是？”
- “告诉我白日依山尽的后续”

难道说要把所有的「提问 - 回答组合」都给 ChatGPT 来做单字接龙吗？

其实不需要。因为训练的主要目的不是记忆，而是学习。

以「单字接龙」的方式来训练「模型」，并不仅仅是为了让「模型」记住某个提问和回答。毕竟在训练之前，数据库已经将所有信息都记忆好了，直接搜索就可以得到答案，没必要先将信息从数据库移动到模型中，再让模型来生成。

之所以不直接搜索，非要训练单字接龙，为的就是让「模型」学习「提问和回答的通用规律」，以便在遇到「从没记忆过的提问」时，也能利用「所学的规律」生成「用户想要的回答」，这种举一反三的目标也叫「泛化」。

例如，当我们用这三个「学习材料」训练 ChatGPT 做单字接龙时：

- “白日依山尽的下一句？**白** 日依山尽的下一句是黄河入海流”
- “白日依山尽的后续内容是？**白** 日依山尽的下一句是黄河入海流”
- “告诉我白日依山尽的后续 **白** 日依山尽的下一句是黄河入海流”

不论面对哪个提问，ChatGPT 都会被要求生成“白日依山尽的下一句是黄河入海流”，这会驱使 ChatGPT 去建构三个提问的通用规律，将自己的模型调整为适用于三个提问的通用模型。

经过这种训练后，即使 ChatGPT 遇到“写出‘白日依山尽’的下一句”这种没记忆过的提问时，依靠「学习后的模型」，就有可能举一反三，也生成“白日依山尽的下一句是黄河入海流”。

很多人都会错误地认为，ChatGPT 是“搜索引擎的升级版本”，是在庞大的数据库中通过超高的运算速度找到最接近的内容，然后进行一些比对和拼接，最终给出结果。

但实际上，ChatGPT 并不具备那种搜索能力。因为在训练过程中，学习材料并不会被保存在模型中，学习材料的作用只是「调整模型」以得到「通用模型」，为的是能处理「未被数据库记忆的情况」。所有结果都是通过「所学到的模型」根据上文，逐字生成的，因此 ChatGPT 也被称为「生成模型」。

「生成模型」与「搜索引擎」非常不同，搜索引擎无法给出「没被数据库记忆的信息」，但生成语言模型可以，还能创造不存在的文本，这正是它的长板，但它却有一些搜索引擎没有的短板。

## 2.4 短板

首先就是：搜索引擎不会混淆记忆，但它有可能会。

为了能应对「未被记忆的情况」，它会学习语言单位（如单词、短语、句子等）之间的规律，用「学到的规律」来成回答，然而，这也意味着如果出现了「实际不同但碰巧符合同一个规律」的内容，模型就可能混淆它。

最直接的结果是：若「现实中不存在内容」刚好符合「它从训练材料中学到的规律」，那 ChatGPT 就有可能对「不存在内容」进行「合乎规律的混合捏造」。

例如，我问它「“三体人为什么害怕大脸猫的威慑，62 年都不敢殖民地球？”」，这个问题并不存在，但又刚好符合「它曾训练过的科幻材料中的规律」，于是它就用「科幻材料中所学到的规律」开始混合捏造了。

这也是为什么，当有人问它事实性内容时，可能会看到它胡说八道。

另一个问题是：它的内容无法被直接增删改查。

不论是 ChatGPT 「所记住的信息」，还是「所学到的规律」，都是以同一个模型的形式来表达的，因此我们无法像操作数据库那样，对这些内容直接进行增删改查。

这会导致两个具体问题：

第一：由于我们很难理解它所建构的规律，又无法直接查看它记住了什么、学到了什么，只能通过多次提问来评估和猜测它的所记所学，其决策缺乏可解释性，这难免会在使用时带来安全风险。

第二：由于只能通过再次调整模型（即再次训练）来增加、删除或修改它的所记所学，这难免在更新时会降低效率。

例如，对于「它编造 [大脸猫](https://www.modevol.com/episode/clf9d5kni0zo301mm6tkl9t87#%E5%A4%A7%E8%84%B8%E7%8C%AB%E5%A8%81%E6%85%91)」的毛病，无法通过直接修改它的回答来矫正，只能通过再训练它做「“三体人为什么害怕大脸猫的威慑？三体人和大脸猫无关”」的单字接龙来调整模型。可这样调整后的效果如何、是否会矫枉过正，或是引入其他问题，又得通过多次提问来评估，容易顾此失彼，效率低下。

还有个特点是：ChatGPT 高度依赖数据，也就是学习材料。

想要让 ChatGPT 能够应对无数未见情况，就必须提供数量足够多、种类足够丰富，质量足够高的学习材料，否则它将无法学到通用规律，给出的回答将会是以偏概全的。

此外，ChatGPT 可能存在的胡编和混淆，也需要用优质的学习材料来修正，所以学习材料非常重要。

之前的 [古诗提问](https://www.modevol.com/episode/clf9d5kni0zo301mm6tkl9t87#%E6%9C%AA%E8%A7%81%E7%9A%84%E6%8F%90%E9%97%AE)，倘若真的仅有三个例子（学习材料），那 ChatGPT 其实也学不到什么通用规律，无法对它没见过的提问做出合理回答，更别提去应对用户的无数奇怪问法了。

总结一下，目前为止视频讲了：

- ChatGPT 的实质功能是单字接龙
- 长文由单字接龙的自回归所生成
- 通过提前训练才能让它生成人们想要的问答
- 训练方式是让它按照问答范例来做单字接龙
- 这样训练是为了让它学会「能举一反三的规律」
- 缺点是可能混淆记忆，无法直接查看和更新所学，且高度依赖学习材料。

> 但你看到这里，可能会想：它也并没有什么特别之处啊，哪有网上说得那么玄乎，基础结构都很简单，为何能火爆到今天这种程度，还要影响整个社会？

## 三步训练

![600](Attachments/8f5ef857436d5470f1bf88c2a5ebe701.png)

### 开卷有益

让机器理解人类语言的一大难点在于：同一个意思可以有多种不同的表达形式，可以用一个词，也可以用一段描述，而同一个表达在不同语境中又有不同含义。

想解决这个问题，就需要让机器学会各种「语义关系」和「语法规律」，以便能明白「哪些表达实际上是同一个意思」。

对此，GPT 的办法是：让模型看到尽可能多、尽可能丰富的「语言范例（学习材料）」，使其有更多机会建构出能举一反三的语言规律，来应对无数「从未见过的语言」。我把这一阶段称为“开卷有益”。GPT 中的“G”代表“生成（Generative）”，“T”代表“Transformer”一种模型结构，而“P”（Pre-training）代表的就是“开卷有益”这一步，专业名称叫“预训练”。

“开卷有益”就好比，在鹦鹉旁边放一台电视机，把各种新闻、国产剧、国外剧、广告、综艺等内容都播给它听。让它自己学，不用人看着。

BERT 也是一种生成语言模型，不同点于，GPT 的学习方式是单字接龙（只允许用上文预测下一个词），而 BERT 的学习方式是完形填空（允许用上下文预测空缺的词）。

然而，“开卷有益”却存在一个问题：尽管 GPT 拥有了海量的知识，但回答形式和内容却不受约束。因为它知道的太多了，见到了一个人几辈子都读不完的资料，会随意联想，**它有能力回答我们的问题，但我们却很难指挥它。**

### 模板规范

**其实解决思路与「我们教鹦鹉对话」的思路是一样的。**

用「对话模板」去矫正它在“开卷有益”时所学到的「不规范“习惯”」。具体做法是：不再用随便的互联网文本，而把人工专门写好的「优质对话范例」给「“开卷有益”后的 GPT-3」，让它再去做单字接龙，从而学习「如何组织符合人类规范的回答」。我把这一阶段称为“模板规范”。

例如，ChatGPT 无法联网，只知道训练数据中的新闻，那么当用户问到最新新闻时，就不应该让它接着续写，而要让它回复“不知道该信息”。

又如，当用户的提问有错误时，也不应该让它顺着瞎编，而要让它指出错误

除了矫正对话方式之外，我们还要防止 GPT-3 补全和续写在“开卷有益”时所学到的「有害内容」，也就是要教它「什么该说，什么不该说」。

大家可能会好奇，**为什么不在一开始就直接教它最正确的对话方式和对话内容呢？**

一方面，「优质对话范例」数量有限，所能提供的语言多样性不足，可能难以让模型学到广泛适用的语言规律，也无法涉猎各个领域。另一方面，「优质对话范例」都需要人工专门标注，价格不菲。这一点其实和「为什么不直接教鹦鹉对话，而是让它先听电视节目」类似。

或许未来，有了足够多的「优质对话范例」后，就会跳过“开卷有益”这一步。

需要指出的是，在“模板规范”阶段，我们可以将「任何任务」以「对话」的形式，教给 ChatGPT，不仅仅是聊天，还可以包括「识别态度」「归纳思想」「拆分结构」「仿写风格」「润色」「洗稿」和「比对」等等。

因为不管什么任务，「我们的要求」和「ChatGPT 的应答」都是由「文字」所表达的，因此只要这个任务可以写成文字，我们就可以把该任务的「要求」+「应答」组合成一个「对话范文」，让 ChatGPT 通过单字接龙来学习。

经过这种“模版规范”后的超大模型，还掌握了两个意外能力：「“理解”指令要求的能力」和「“理解”例子要求的能力」。

#### 理解指令要求

「“理解”指令要求」是指「能按照用户的抽象描述，给出处理结果」。  
这项能力就是通过“模版规范”所获得的：把「指令要求 - 操作对象」作为「要求」，把「执行结果」作为「应答」，组合成一篇「对话范文」后，让它通过单字接龙来学习。

例如，给它下面几个「对话范文」来做单字接龙：

- “将下文翻译成中文 apple 苹果”
- “翻译成中文 apple 苹果”
- “翻译 apple 苹果”

ChatGPT 就能学会「“翻译”这个指令」。

#### 理解例子要求

「“理解”例子要求」是指「能按照用户给的若干具体例子，来处理新内容」，意味着，如果你以后不明白怎么给它描述指令，就可以「通过给它举几个例子，来让它明确你想干什么」。

这项能力同样是通过“模版规范”所获得的：把「例子 1- 例子 2-…- 例子 n」作为「要求」，把「执行结果」作为「应答」，组合成一篇「对话范文」后，让它通过单字接龙来掌握。

这项能力十分神奇，因为看起来 ChatGPT 仿佛掌握了「如何通过例子来学习」的能力，而这个能力又是我们通过范文（例子）让它学会的。产生了一种“它学会了如何学习”的套娃感。大家把这种现象称为“**语境内学习（In-context Learning）**”，目前对这种能力的产生原因还没有定论。

#### 分治效应

在超大模型的使用中，大家还发现了一种「分治效应」：当 ChatGPT 无法答对一个综合问题时，若要求它分步思考，它就可以一步步连续推理，且最终答对的可能性大幅提升，该能力也叫“思维链”。

ChatGPT 的思维链（chain of thought）能力，可能是在训练做代码的单字接龙后所产生的。因为人类在面对复杂任务时，直接思考答案也会没头绪，用分而治之往往可以解决。因此大家猜测，ChatGPT 可能是通过对代码的单字接龙，学到了代码中所蕴含的「人类分治思想」。不过目前对该现象的产生原因也没有定论。

### 创意引导 -[[RLHF]]

那么，如何在维持人类对话模式和价值取向的前提下，提高 ChatGPT 的创新性呢？  

可以联想一下鹦鹉是怎么被训练的。当我们教会鹦鹉一些基本对话后，就可以让鹦鹉自由发挥，有时鹦鹉会蹦出一些非常有意思的对话，这时我们就可以给它吃的，强化它在该方向的行为。

在训练 ChatGPT 的第三阶段，也是类似的过程。  
这一次，不再要求它按照我们提供的对话范例做单字接龙，而是直接向它提问，再让它自由回答。如果回答得妙，就给予奖励，如果回答不佳，就降低奖励。然后利用这些「人类评分」去调整 ChatGPT 的模型。  
在这种训练中，我们既不会用现有的模板来限制它的表现，又可以引导它创造出符合人类认可的回答。我把这一阶段称为“创意引导”。

### 总结

- “开卷有益”阶段：让 ChatGPT 对「海量互联网文本」做单字接龙，以扩充模型的词汇量、语言知识、世界的信息与知识。使 ChatGPT 从“哑巴鹦鹉”变成“脑容量超级大的懂王鹦鹉”。
- “模板规范”阶段：让 ChatGPT 对「优质对话范例」做单字接龙，以规范回答的对话模式和对话内容。使 ChatGPT 变成“懂规矩的博学鹦鹉”。
- “创意引导”阶段：让 ChatGPT 根据「人类对它生成答案的好坏评分」来调节模型，以引导它生成人类认可的创意回答。使 ChatGPT 变成“既懂规矩又会试探的博学鹦鹉”。
