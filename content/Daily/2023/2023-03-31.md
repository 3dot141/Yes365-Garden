---
aliases: []
created_date: 2023-03-31 00:00
draft: false
summary: ''
tags:
- daily
---

```dataview
TABLE dateformat(file.ctime,"yyyy-MM-dd HH:mm:ss") as "created", file.size as "size" WHERE this.file.name = dateformat(file.ctime,"yyyy-MM-dd")
SORT file.ctime ASC
```

# RunnerGO

- [RunnerGo-全栈测试平台-开源性能测试工具](https://www.runnergo.com/)

## 基于 go 语言的一体化性能压测工具

RunnerGo 致力于打造成一款全栈式测试平台，采用了较为宽松的 Apache-2.0 license 开源协议，方便志同道合的朋友一起为开源贡献力量，目前实现了接口测试、场景自动化测试、性能测试等测试能力。随着不断的迭代，我们将会推出更多的测试功能。我们的目的是为研发赋能，让测试更简单。

## 工具特性

- go 语言运行：基于 go 语言开发，运行速度快、更节省资源
- 智能调度算法：自研的调度算法，合理利用服务器资源，降低资源消耗
- 实时生成测试报告：运行任务后，可实时查看执行结果，快速诊断服务病症
- 丰富的报告图表： 全方位展示各个指标运行曲线图
- 实时修改： 可根据压测模式实时修改并发数、持续时长等
- 实时日志： 可在压测过程中开启日志模式，查看请求响应信息
- 可编辑报告：可在任务运行结束后，针对测试结果进行测试分析，实时编写报告
- Flow 场景流：可视化的业务流，通过连线就可快速搭建起来自己的业务流，还可直接调试运行场景，电流般的业务流转
- 多种压测模式：支持并发模式、阶梯模式、错误率模式、响应时间模式、每秒应答数模式等多种压测模式，满足所有业务需求
- 自持接口自动化，采用用例集概念，生成丰富的自动化报告

# ChatGPT-UI

> 聊天机器人的开源 UI

- [GitHub - ourongxing/chatgpt-vercel: Elegant and Powerfull. Powered by OpenAI and Vercel.](https://github.com/ourongxing/chatgpt-vercel)
	- [Vercel](../../Outputs/Card/Vercel.md) 本身存在科学上网的问题
	- 如果你只需要部署一个你自己用的网站，而不需要定制，那么你完全不需要在本地跑起来，你可以直接点击下面的按钮，然后按照提示操作，然后在 Vercel 中填入环境变量即可。vercel.app 域名已经被墙，但 vercel 本身没有被墙，所以你绑定自己的域名就可以了。如果广泛分享，域名有被墙的风险。
- [GitHub - Yidadaa/ChatGPT-Next-Web: 一键拥有你自己的 ChatGPT 网页服务。 One-Click to deploy your own ChatGPT web UI.](https://github.com/Yidadaa/ChatGPT-Next-Web)

# Colossal-AI

> [GitHub - hpcaitech/ColossalAI: Making large AI models cheaper, faster and more accessible](https://github.com/hpcaitech/ColossalAI)

- [ColossalChat: An Open-Source Solution for Cloning ChatGPT With a Complete RLHF Pipeline | by Yang You | Mar, 2023 | Medium](https://medium.com/@yangyou_berkeley/colossalchat-an-open-source-solution-for-cloning-chatgpt-with-a-complete-rlhf-pipeline-5edf08fb538b)
- 提供一套完成的 [RLHF](../../Inputs/Article/RLHF.md) 的流程
## Low-cost Fine-tuning of LoRA

Colossal-AI includes the Low-Rank Adaptation (LoRA) method for low-cost fine-tuning of large models. The LoRA method assumes that large language models are over-parameterized and that the parameter change during fine-tuning is a low-rank matrix. Therefore, this matrix can be decomposed into the product of two smaller matrices. During fine-tuning, the parameters of the large model are fixed, and only the parameters of the low-rank matrix are adjusted, significantly reducing the number of parameters required for training and lowering the cost.

> 翻译一下
> 
> Colossal-AI 包括用于对大型模型进行低成本微调的低秩适应 (LoRA) 方法。 LoRA 方法假设大型语言模型是过度参数化的，并且微调期间的参数变化是一个低秩矩阵。因此，这个矩阵可以分解为两个更小的矩阵的乘积。 fine-tuning 时固定大模型的参数，只调整低秩矩阵的参数，显着减少训练所需的参数数量，降低成本。

# [RLHF](../../Inputs/Article/RLHF.md) 模型评测

> [GitHub - sunzeyeah/RLHF: Implementation of Chinese ChatGPT](https://github.com/sunzeyeah/RLHF)

提供 2 大功能：
- LLM 模型评测：参考 GPT 类模型，基于 ZeroShot 和 FewShot 实现
- ChatGPT 模型训练 pipeline：根据 [Learning to Summarize from human feedback](https://arxiv.org/abs/2009.01325) ，实现 3 大流程: SFT、Reward Model 和 [RLHF](../../Inputs/Article/RLHF.md)